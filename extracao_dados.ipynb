{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d003d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 12.0/12.0 MB 75.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 566.1/566.1 kB 19.0 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 77.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.36.0 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "#!pip install SPARQLWrapper\n",
    "#!pip install SPARQLWrapper\n",
    "#!pip install rdflib\n",
    "#!pip install pyvis\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b407b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "import os\n",
    "from rdflib import Graph, Namespace, RDF\n",
    "from pyvis.network import Network\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09feab67",
   "metadata": {},
   "source": [
    "### Extração do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d726caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36897, 6)\n",
      "(1000, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# extrair o dataset\n",
    "df = pd.read_parquet(\"hf://datasets/manoh2f2/tsterbak-lyrics-dataset-with-emotions/data/train-00000-of-00001.parquet\")\n",
    "print(df.shape)\n",
    "\n",
    "# shuffle dos dados e reseta também o indice\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# redução do dataset\n",
    "df = df.iloc[0:1000, :]\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "#tratamento do problema das letras ter este _x000D_ caracter especial\n",
    "df['seq'] = df['seq'].str.replace(\"_x000D_\", \"\", regex=False)\n",
    "\n",
    "\n",
    "# print(df)\n",
    "\n",
    "#x = df.iloc[2]['seq']\n",
    "#print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab8d6f",
   "metadata": {},
   "source": [
    "### Query do SPARQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a12d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Total de artistas únicos: 642\n"
     ]
    }
   ],
   "source": [
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "cache_file = \"genre_cache.json\"\n",
    "# cache_str_keys é do tipo { \"song||artist\": genre }\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cache_str_keys = json.load(f)\n",
    "        print(len(cache_str_keys))\n",
    "else:\n",
    "    cache_str_keys = {}\n",
    "    with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache_str_keys, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def count_unique_artists(cache_file):\n",
    "    with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # extrai o artista da chave \"song||artist\"\n",
    "    artists = [key.split(\"||\")[1] for key in data.keys()]\n",
    "    unique_artists = set(artists)\n",
    "    \n",
    "    print(f\"Total de artistas únicos: {len(unique_artists)}\")\n",
    "    return unique_artists\n",
    "\n",
    "\n",
    "def get_genre2(artist_name):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?genreLabel WHERE {{\n",
    "      ?artist rdfs:label \"{artist_name}\"@en;\n",
    "              wdt:P136 ?genre.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        return results[\"results\"][\"bindings\"][0][\"genreLabel\"][\"value\"]\n",
    "\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def get_genre(song_name,artist_name):\n",
    "    \n",
    "    song_name = song_name.replace('\"', '').replace(\"'\", '')\n",
    "    artist_name = artist_name.replace('\"', '').replace(\"'\", '')\n",
    "    query = f\"\"\"\n",
    "    SELECT ?genreLabel WHERE {{\n",
    "      ?artist rdfs:label \"{song_name}\"@en;\n",
    "            wdt:P31 wd:Q7366;          # garante que é uma canção\n",
    "            wdt:P136 ?genre.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        return results[\"results\"][\"bindings\"][0][\"genreLabel\"][\"value\"]\n",
    "\n",
    "    else:\n",
    "        return get_genre2(artist_name) # Caso não encontre género para a música em especifico, vai buscar o género do artista\n",
    "\n",
    "\n",
    "\n",
    "def get_genre_cached(song_name, artist_name):\n",
    "    key = f\"{song_name}||{artist_name}\"  # chave como string\n",
    "    if key in cache_str_keys:\n",
    "        return cache_str_keys[key]\n",
    "    \n",
    "    genre = get_genre(song_name, artist_name)  # faz SPARQL se não estiver\n",
    "    cache_str_keys[key] = genre\n",
    "\n",
    "    # salva a cache imediatamente no JSON\n",
    "    with open(\"genre_cache.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache_str_keys, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return genre\n",
    "\n",
    "\n",
    "\n",
    "# artist = df.iloc[2][\"artist\"]\n",
    "# song = df.iloc[2][\"song\"]\n",
    "# genre = get_genre(song,artist)\n",
    "\n",
    "# print(f\"Artista: {artist}\")\n",
    "# print(f\"Género: {genre}\")\n",
    "\n",
    "#df[\"genre\"] = df[\"artist\"].apply(get_genre)\n",
    "\n",
    "\n",
    "count_unique_artists(cache_file)\n",
    "df[\"genre\"] = [get_genre_cached(title, artist) for title, artist in zip(df[\"song\"], df[\"artist\"])]\n",
    "\n",
    "\n",
    "df.to_csv(\"lyrics_with_genre.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness\n",
      "0.8350851535797119\n",
      "0.10378336906433105\n",
      "0.03163175284862518\n",
      "fear\n",
      "0.6831692457199097\n",
      "0.17267188429832458\n",
      "0.07465752214193344\n",
      "fear\n",
      "0.4248541295528412\n",
      "0.21741630136966705\n",
      "0.1181325763463974\n",
      "fear\n",
      "0.5999545454978943\n",
      "0.32163679599761963\n",
      "0.039006754755973816\n",
      "anger\n",
      "0.47958657145500183\n",
      "0.2513619363307953\n",
      "0.15195214748382568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_9676\\2573901906.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataHead['predicted_emotions'] = emotion_list\n"
     ]
    }
   ],
   "source": [
    "def emotionSelection(i, lista):\n",
    "        lista_sent = list(lista[i].keys())\n",
    "        first_key = lista_sent[0]\n",
    "        first_key_score = lista[i][first_key]\n",
    "        print(first_key)\n",
    "        for j in lista_sent:\n",
    "            score = lista[i][j]\n",
    "            print(score)\n",
    "            if(score < first_key_score/2):\n",
    "                del emotion_list[i][j]                \n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k = 3,truncation=True) #truncation faz com que corte quando for muito longo\n",
    "\n",
    "data = pd.read_csv(\"lyrics_with_genre.csv\")\n",
    "data['seq'] = data['seq'].str.replace(\"_x000D_\", \"\", regex=False)\n",
    "\n",
    "#dataHead = data.head()\n",
    "#print(dataHead)\n",
    "\n",
    "#x = dataHead.iloc[0]['seq']\n",
    "\n",
    "#print(classifier(x))\n",
    "#print(dataHead.iloc[0]['emotions'])\n",
    "#print(x)\n",
    "\n",
    "emotion_list = []\n",
    "\n",
    "for i,row in data.iterrows():\n",
    "    temp = row['seq']\n",
    "\n",
    "    #print(classifier(temp))\n",
    "\n",
    "    prediction = classifier(temp)\n",
    "    inner = prediction[0]   # devolve a lista interna\n",
    "    d = {item['label']: item['score'] for item in inner}\n",
    "\n",
    "    emotion_list.append(d)\n",
    "    emotionSelection(i,emotion_list)\n",
    "\n",
    "#print(emotion_list[0].keys())\n",
    "#first_key = list(emotion_list[0].keys())[0]\n",
    "#print(first_key)\n",
    "#del emotion_list[0]['fear']\n",
    "#print(emotion_list[0].keys())\n",
    "\n",
    "data['predicted_emotions'] = emotion_list\n",
    "\n",
    "\n",
    "data.to_csv(\"final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a6f4b",
   "metadata": {},
   "source": [
    "### Gráfico de conhecimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c3e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rdflib import Graph, Namespace, RDF\n",
    "from pyvis.network import Network\n",
    "import ast\n",
    "\n",
    "data = pd.read_csv(\"final.csv\")\n",
    "\n",
    "data = data.head(25)\n",
    "\n",
    "# FUNÇÃO PARA LIMPAR URIs\n",
    "def clean_uri(x):\n",
    "    return x.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\"\", \"\").replace(\"'\", \"_\")\n",
    "\n",
    "# CRIAR GRAFO RDF\n",
    "g = Graph()\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "g.bind(\"\", EX)\n",
    "\n",
    "# DEFINIR CLASSES\n",
    "g.add((EX.Music, RDF.type, EX.Class))\n",
    "g.add((EX.Artist, RDF.type, EX.Class))\n",
    "g.add((EX.Genre, RDF.type, EX.Class))\n",
    "g.add((EX.Emotion, RDF.type, EX.Class))\n",
    "\n",
    "# DEFINIR PROPRIEDADES\n",
    "g.add((EX.hasArtist, RDF.type, EX.Property))\n",
    "g.add((EX.hasGenre, RDF.type, EX.Property))\n",
    "g.add((EX.hasEmotion, RDF.type, EX.Property))\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "\n",
    "    title_uri   = EX[clean_uri(row[\"song\"])]\n",
    "    artist_uri  = EX[clean_uri(row[\"artist\"])]\n",
    "    genre_uri   = EX[clean_uri(row[\"genre\"])]\n",
    "\n",
    "    # --- converter o texto '{'sadness':0.83}' num dicionário ---\n",
    "    emotions_dict = ast.literal_eval(row[\"predicted_emotions\"])\n",
    "\n",
    "    # --- criar URIs para todas as emoções ---\n",
    "    emotion_uris = [EX[clean_uri(em)] for em in emotions_dict.keys()]\n",
    "\n",
    "    # tipos das instâncias\n",
    "    g.add((title_uri, RDF.type, EX.Music))\n",
    "    g.add((artist_uri, RDF.type, EX.Artist))\n",
    "    g.add((genre_uri, RDF.type, EX.Genre))\n",
    "\n",
    "    for em_uri in emotion_uris:\n",
    "        g.add((em_uri, RDF.type, EX.Emotion))\n",
    "\n",
    "    # relações\n",
    "    g.add((title_uri, EX.hasArtist, artist_uri))\n",
    "    g.add((title_uri, EX.hasGenre, genre_uri))\n",
    "\n",
    "    # ligar música → todas as emoções\n",
    "    for em_uri in emotion_uris:\n",
    "        g.add((title_uri, EX.hasEmotion, em_uri))\n",
    "\n",
    "g.serialize(\"musicas.ttl\", format=\"turtle\")\n",
    "\n",
    "g = Graph()\n",
    "g.parse(\"musicas.ttl\", format=\"turtle\")\n",
    "\n",
    "# --- preparar visualização PyVis com processamento mais limpo ---\n",
    "net = Network(height=\"750px\", width=\"100%\", directed=True)\n",
    "net.barnes_hut()  # layout melhor para grafos maiores\n",
    "\n",
    "# iremos recolher tipos (Music / Artist / Genre) e depois construir nós/arestas sem triples rdf:type visíveis\n",
    "class_uris = {EX.Music, EX.Artist, EX.Genre, EX.Emotion}\n",
    "node_type = {}   # mapa: URI -> 'Music'|'Artist'|'Genre'|'Other'\n",
    "\n",
    "# Identificar tipos\n",
    "for s, p, o in g:\n",
    "    if p == RDF.type and o in class_uris:\n",
    "        if o == EX.Music:\n",
    "            node_type[s] = \"Music\"\n",
    "        elif o == EX.Artist:\n",
    "            node_type[s] = \"Artist\"\n",
    "        elif o == EX.Genre:\n",
    "            node_type[s] = \"Genre\"\n",
    "        elif o == EX.Emotion:\n",
    "            node_type[s] = \"Emotion\"\n",
    "\n",
    "# depois criar nós e arestas (ignorando triples que apenas declaram as classes em si)\n",
    "seen_nodes = set()\n",
    "\n",
    "def pretty_label(uri):\n",
    "    \"\"\"Gera label legível: tenta qname, fallback para o último segmento do URIRef\"\"\"\n",
    "    try:\n",
    "        return g.qname(uri)\n",
    "    except Exception:\n",
    "        s = str(uri)\n",
    "        return s.split(\"/\")[-1].split(\"#\")[-1]\n",
    "\n",
    "# cores/grupos para PyVis (o \"group\" facilita legenda/estética)\n",
    "group_map = {\n",
    "    \"Music\": \"music\",\n",
    "    \"Artist\": \"artist\",\n",
    "    \"Genre\": \"genre\",\n",
    "    \"Emotion\": \"emotion\"\n",
    "}\n",
    "\n",
    "# Adicionar nós e arestas: para cada triple, se for rdf:type (instância->classe) já processado -> ignorar visualmente.\n",
    "for s, p, o in g:\n",
    "    # Só mostrar indivíduos, não classes\n",
    "    if o in (EX.Music, EX.Artist, EX.Genre):\n",
    "        continue\n",
    "    # ignorar declarações do próprio esquema (ex.: EX.Music rdf:type EX.Class) e rdf:type ligações já processadas\n",
    "    if s in {EX.Music, EX.Artist, EX.Genre}:\n",
    "        continue\n",
    "    if p == RDF.type and o in class_uris:\n",
    "        # Não criar aresta rdf:type visível — apenas asseguramos node_type acima\n",
    "        continue\n",
    "\n",
    "    # garantir nós s e o com labels legíveis\n",
    "    if s not in seen_nodes:\n",
    "        lbl = pretty_label(s)\n",
    "        grp = group_map.get(node_type.get(s, \"Other\"), \"other\")\n",
    "        net.add_node(str(s), label=lbl, title=str(s), group=grp)\n",
    "        seen_nodes.add(s)\n",
    "    if o not in seen_nodes:\n",
    "        lbl = pretty_label(o)\n",
    "        grp = group_map.get(node_type.get(o, \"Other\"), \"other\")\n",
    "        net.add_node(str(o), label=lbl, title=str(o), group=grp)\n",
    "        seen_nodes.add(o)\n",
    "\n",
    "    # adicionar aresta com rótulo do predicado (localname)\n",
    "    pred_label = pretty_label(p)\n",
    "    net.add_edge(str(s), str(o), label=pred_label, title=pred_label)\n",
    "\n",
    "\n",
    "# PyVis aplica cores automaticamente por group.\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"nodes\": {\n",
    "    \"font\": {\"size\": 14}\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"arrows\": {\"to\": {\"enabled\": true}},\n",
    "    \"font\": {\"align\": \"top\"}\n",
    "  },\n",
    "  \"physics\": {\n",
    "    \"stabilization\": { \"enabled\": true }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "net.write_html(\"grafico_interativo_limpo.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a14106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.30%\n",
      "F1-score: 78.34%\n",
      "783 / 1000 correspondências\n"
     ]
    }
   ],
   "source": [
    "# Lê o ficheiro CSV\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "\n",
    "def clean_emotion_list(x):\n",
    "    try:\n",
    "        lst = ast.literal_eval(x)     # converte a string ['sadness'] para lista real\n",
    "        return lst[0].strip().lower()\n",
    "    except:\n",
    "        return str(x).strip().lower()\n",
    "\n",
    "\n",
    "df[\"emotions_clean\"] = df[\"emotions\"].apply(clean_emotion_list)\n",
    "\n",
    "# --- LIMPAR predicted_emotions: {'sadness': 0.83} → sadness ---\n",
    "def clean_predicted_dict(x):\n",
    "    try:\n",
    "        d = ast.literal_eval(x)       # converte string dict → dict real\n",
    "        return max(d, key=d.get).strip().lower()   # escolhe a emoção com maior score\n",
    "    except:\n",
    "        return str(x).strip().lower()\n",
    "\n",
    "df[\"pred_clean\"] = df[\"predicted_emotions\"].apply(clean_predicted_dict)\n",
    "\n",
    "#df[\"emotions_clean\"] = df[\"emotions\"].astype(str).str.strip().str.lower()\n",
    "#df[\"pred_clean\"] = df[\"predicted_emotions\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "y_true = df[\"emotions_clean\"]\n",
    "y_pred = df[\"pred_clean\"]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")  \n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1 * 100:.2f}%\")\n",
    "\n",
    "matches = (df[\"emotions_clean\"] == df[\"pred_clean\"]).sum()\n",
    "total = len(df)\n",
    "print(matches, \"/\", total, \"correspondências\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
