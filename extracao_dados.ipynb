{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d003d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b407b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "import os\n",
    "from rdflib import Graph, Namespace, RDF\n",
    "from pyvis.network import Network\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09feab67",
   "metadata": {},
   "source": [
    "### Extração do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d726caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36897, 6)\n",
      "(1000, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# extrair o dataset\n",
    "df = pd.read_parquet(\"hf://datasets/manoh2f2/tsterbak-lyrics-dataset-with-emotions/data/train-00000-of-00001.parquet\")\n",
    "print(df.shape)\n",
    "\n",
    "# shuffle dos dados e reseta também o indice\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# redução do dataset\n",
    "df = df.iloc[0:1000, :]\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "#tratamento do problema das letras ter este _x000D_ caracter especial\n",
    "df['seq'] = df['seq'].str.replace(\"_x000D_\", \"\", regex=False)\n",
    "\n",
    "\n",
    "# print(df)\n",
    "\n",
    "#x = df.iloc[2]['seq']\n",
    "#print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab8d6f",
   "metadata": {},
   "source": [
    "### Query do SPARQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a12d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Total de artistas únicos: 642\n"
     ]
    }
   ],
   "source": [
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "cache_file = \"genre_cache.json\"\n",
    "# cache_str_keys é do tipo { \"song||artist\": genre }\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cache_str_keys = json.load(f)\n",
    "        print(len(cache_str_keys))\n",
    "else:\n",
    "    cache_str_keys = {}\n",
    "    with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache_str_keys, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def count_unique_artists(cache_file):\n",
    "    with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # extrai o artista da chave \"song||artist\"\n",
    "    artists = [key.split(\"||\")[1] for key in data.keys()]\n",
    "    unique_artists = set(artists)\n",
    "    \n",
    "    print(f\"Total de artistas únicos: {len(unique_artists)}\")\n",
    "    return unique_artists\n",
    "\n",
    "\n",
    "def get_genre2(artist_name):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?genreLabel WHERE {{\n",
    "      ?artist rdfs:label \"{artist_name}\"@en;\n",
    "              wdt:P136 ?genre.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        return results[\"results\"][\"bindings\"][0][\"genreLabel\"][\"value\"]\n",
    "\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def get_genre(song_name,artist_name):\n",
    "    \n",
    "    song_name = song_name.replace('\"', '').replace(\"'\", '')\n",
    "    artist_name = artist_name.replace('\"', '').replace(\"'\", '')\n",
    "    query = f\"\"\"\n",
    "    SELECT ?genreLabel WHERE {{\n",
    "      ?artist rdfs:label \"{song_name}\"@en;\n",
    "            wdt:P31 wd:Q7366;          # garante que é uma canção\n",
    "            wdt:P136 ?genre.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        return results[\"results\"][\"bindings\"][0][\"genreLabel\"][\"value\"]\n",
    "\n",
    "    else:\n",
    "        return get_genre2(artist_name) # Caso não encontre género para a música em especifico, vai buscar o género do artista\n",
    "\n",
    "\n",
    "\n",
    "def get_genre_cached(song_name, artist_name):\n",
    "    key = f\"{song_name}||{artist_name}\"  # chave como string\n",
    "    if key in cache_str_keys:\n",
    "        return cache_str_keys[key]\n",
    "    \n",
    "    genre = get_genre(song_name, artist_name)  # faz SPARQL se não estiver\n",
    "    cache_str_keys[key] = genre\n",
    "\n",
    "    # salva a cache imediatamente no JSON\n",
    "    with open(\"genre_cache.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache_str_keys, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return genre\n",
    "\n",
    "\n",
    "\n",
    "# artist = df.iloc[2][\"artist\"]\n",
    "# song = df.iloc[2][\"song\"]\n",
    "# genre = get_genre(song,artist)\n",
    "\n",
    "# print(f\"Artista: {artist}\")\n",
    "# print(f\"Género: {genre}\")\n",
    "\n",
    "#df[\"genre\"] = df[\"artist\"].apply(get_genre)\n",
    "\n",
    "\n",
    "count_unique_artists(cache_file)\n",
    "df[\"genre\"] = [get_genre_cached(title, artist) for title, artist in zip(df[\"song\"], df[\"artist\"])]\n",
    "\n",
    "\n",
    "df.to_csv(\"lyrics_with_genre.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness\n",
      "0.8350851535797119\n",
      "0.10378336906433105\n",
      "0.03163175284862518\n",
      "fear\n",
      "0.6831692457199097\n",
      "0.17267188429832458\n",
      "0.07465752214193344\n",
      "fear\n",
      "0.4248541295528412\n",
      "0.21741630136966705\n",
      "0.1181325763463974\n",
      "fear\n",
      "0.5999545454978943\n",
      "0.32163679599761963\n",
      "0.039006754755973816\n",
      "anger\n",
      "0.47958657145500183\n",
      "0.2513619363307953\n",
      "0.15195214748382568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_9676\\2573901906.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataHead['predicted_emotions'] = emotion_list\n"
     ]
    }
   ],
   "source": [
    "def emotionSelection(i, lista):\n",
    "        lista_sent = list(lista[i].keys())\n",
    "        first_key = lista_sent[0]\n",
    "        first_key_score = lista[i][first_key]\n",
    "        print(first_key)\n",
    "        for j in lista_sent:\n",
    "            score = lista[i][j]\n",
    "            print(score)\n",
    "            if(score < first_key_score/2):\n",
    "                del emotion_list[i][j]                \n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k = 3,truncation=True) #truncation faz com que corte quando for muito longo\n",
    "\n",
    "data = pd.read_csv(\"lyrics_with_genre.csv\")\n",
    "data['seq'] = data['seq'].str.replace(\"_x000D_\", \"\", regex=False)\n",
    "\n",
    "#dataHead = data.head()\n",
    "#print(dataHead)\n",
    "\n",
    "#x = dataHead.iloc[0]['seq']\n",
    "\n",
    "#print(classifier(x))\n",
    "#print(dataHead.iloc[0]['emotions'])\n",
    "#print(x)\n",
    "\n",
    "emotion_list = []\n",
    "\n",
    "for i,row in data.iterrows():\n",
    "    temp = row['seq']\n",
    "\n",
    "    #print(classifier(temp))\n",
    "\n",
    "    prediction = classifier(temp)\n",
    "    inner = prediction[0]   # devolve a lista interna\n",
    "    d = {item['label']: item['score'] for item in inner}\n",
    "\n",
    "    emotion_list.append(d)\n",
    "    emotionSelection(i,emotion_list)\n",
    "\n",
    "#print(emotion_list[0].keys())\n",
    "#first_key = list(emotion_list[0].keys())[0]\n",
    "#print(first_key)\n",
    "#del emotion_list[0]['fear']\n",
    "#print(emotion_list[0].keys())\n",
    "\n",
    "data['predicted_emotions'] = emotion_list\n",
    "\n",
    "\n",
    "data.to_csv(\"final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a6f4b",
   "metadata": {},
   "source": [
    "### Gráfico de conhecimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"I Was Born About Ten Thousand Years Ago||Elvis Presley\": \"rock and roll\",\n",
    "  \"Citadel||The Damned\": \"drama film\",\n",
    "  \"Down the Drain||Down by Law\": \"drama film\",\n",
    "  \"Hymn||Patti Smith\": \"Unknown\",\n",
    "  \"Candy||LL Cool J\": \"pop music\",\n",
    "  \"Little Birds||Dead to Fall\": \"indie rock\",\n",
    "  \"Hannah||Sheila Nicholls\": \"Unknown\",\n",
    "  \"Mental Slavery||Kreator\": \"thrash metal\",\n",
    "  \"Playin' Dominoes and Shootin' Dice||Willie Nelson\": \"blues\"\n",
    "}\n",
    "\n",
    "# Criar o grafo\n",
    "g = Graph()\n",
    "\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "g.bind(\"\", EX)\n",
    "\n",
    "# Criar classes\n",
    "g.add((EX.Music, RDF.type, EX.Class))\n",
    "g.add((EX.Artist, RDF.type, EX.Class))\n",
    "g.add((EX.Genre, RDF.type, EX.Class))\n",
    "#TODO: criar classe sentimento\n",
    "\n",
    "# Criar propriedades\n",
    "g.add((EX.hasArtist, RDF.type, EX.Property))\n",
    "g.add((EX.hasGenre, RDF.type, EX.Property))\n",
    "#TODO: criar propriedade tem sentimento\n",
    "\n",
    "# --------- ADICIONAR INDIVÍDUOS ---------\n",
    "for key, genre in data.items():\n",
    "    title, artist = key.split(\"||\")\n",
    "\n",
    "    # Criar URIs \"limpos\"\n",
    "    title_uri = EX[title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\"\", \"\").replace(\"'\", \"_\")]\n",
    "    artist_uri = EX[artist.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\"\", \"\")]\n",
    "    genre_uri = EX[genre.replace(\" \", \"_\").replace(\"/\", \"_\")]\n",
    "\n",
    "    # Música é indivíduo da classe Music\n",
    "    g.add((title_uri, RDF.type, EX.Music))\n",
    "\n",
    "    # Artista é indivíduo da classe Artist\n",
    "    g.add((artist_uri, RDF.type, EX.Artist))\n",
    "\n",
    "    # Género é indivíduo da classe Genre\n",
    "    g.add((genre_uri, RDF.type, EX.Genre))\n",
    "\n",
    "    # Relacionar a música com artista e género\n",
    "    g.add((title_uri, EX.hasArtist, artist_uri))\n",
    "    g.add((title_uri, EX.hasGenre, genre_uri))\n",
    "\n",
    "g.serialize(\"musicas.ttl\", format=\"turtle\")\n",
    "\n",
    "# Carregar o teu grafo RDFLib\n",
    "g = Graph()\n",
    "g.parse(\"musicas.ttl\", format=\"turtle\")\n",
    "\n",
    "# --- preparar visualização PyVis com processamento mais limpo ---\n",
    "net = Network(height=\"750px\", width=\"100%\", directed=True)\n",
    "net.barnes_hut()  # layout melhor para grafos maiores\n",
    "\n",
    "# iremos recolher tipos (Music / Artist / Genre) e depois construir nós/arestas sem triples rdf:type visíveis\n",
    "class_uris = {EX.Music, EX.Artist, EX.Genre}\n",
    "node_type = {}   # mapa: URI -> 'Music'|'Artist'|'Genre'\n",
    "\n",
    "# primeiro passar pelos triples para identificar rdf:type de instâncias\n",
    "for s, p, o in g:\n",
    "    if p == RDF.type and o in class_uris:\n",
    "        # marca o tipo da instância (s é a instância, o é a classe)\n",
    "        if o == EX.Music:\n",
    "            node_type[s] = \"Music\"\n",
    "        elif o == EX.Artist:\n",
    "            node_type[s] = \"Artist\"\n",
    "        elif o == EX.Genre:\n",
    "            node_type[s] = \"Genre\"\n",
    "\n",
    "# depois criar nós e arestas (ignorando triples que apenas declaram as classes em si)\n",
    "seen_nodes = set()\n",
    "\n",
    "def pretty_label(uri):\n",
    "    \"\"\"Gera label legível: tenta qname, fallback para o último segmento do URIRef\"\"\"\n",
    "    try:\n",
    "        return g.qname(uri)\n",
    "    except Exception:\n",
    "        s = str(uri)\n",
    "        return s.split(\"/\")[-1].split(\"#\")[-1]\n",
    "\n",
    "# cores/grupos para PyVis (o \"group\" facilita legenda/estética)\n",
    "group_map = {\n",
    "    \"Music\": \"music\",\n",
    "    \"Artist\": \"artist\",\n",
    "    \"Genre\": \"genre\"\n",
    "}\n",
    "\n",
    "# Adicionar nós e arestas: para cada triple, se for rdf:type (instância->classe) já processado -> ignorar visualmente.\n",
    "for s, p, o in g:\n",
    "    # Só mostrar indivíduos, não classes\n",
    "    if o in (EX.Music, EX.Artist, EX.Genre):\n",
    "        continue\n",
    "    # ignorar declarações do próprio esquema (ex.: EX.Music rdf:type EX.Class) e rdf:type ligações já processadas\n",
    "    if s in {EX.Music, EX.Artist, EX.Genre}:\n",
    "        continue\n",
    "    if p == RDF.type and o in class_uris:\n",
    "        # Não criar aresta rdf:type visível — apenas asseguramos node_type acima\n",
    "        continue\n",
    "\n",
    "    # garantir nós s e o com labels legíveis\n",
    "    if s not in seen_nodes:\n",
    "        lbl = pretty_label(s)\n",
    "        grp = group_map.get(node_type.get(s, \"Other\"), \"other\")\n",
    "        net.add_node(str(s), label=lbl, title=str(s), group=grp)\n",
    "        seen_nodes.add(s)\n",
    "    if o not in seen_nodes:\n",
    "        lbl = pretty_label(o)\n",
    "        grp = group_map.get(node_type.get(o, \"Other\"), \"other\")\n",
    "        net.add_node(str(o), label=lbl, title=str(o), group=grp)\n",
    "        seen_nodes.add(o)\n",
    "\n",
    "    # adicionar aresta com rótulo do predicado (localname)\n",
    "    pred_label = pretty_label(p)\n",
    "    net.add_edge(str(s), str(o), label=pred_label, title=pred_label)\n",
    "\n",
    "\n",
    "# PyVis aplica cores automaticamente por group.\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"nodes\": {\n",
    "    \"font\": {\"size\": 14}\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"arrows\": {\"to\": {\"enabled\": true}},\n",
    "    \"font\": {\"align\": \"top\"}\n",
    "  },\n",
    "  \"physics\": {\n",
    "    \"stabilization\": { \"enabled\": true }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "net.write_html(\"grafico_interativo_limpo.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
